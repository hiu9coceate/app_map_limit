import 'dart:io';
import 'dart:typed_data';
import 'package:flutter/services.dart';
import 'package:image/image.dart' as img;
import 'package:onnxruntime/onnxruntime.dart';
import 'package:path_provider/path_provider.dart';
import 'package:path/path.dart' as path;
import '../../domain/entities/detection_result.dart';

/// Service ƒë·ªÉ ph√°t hi·ªán bi·ªÉn b√°o giao th√¥ng s·ª≠ d·ª•ng ONNX Runtime
class OnnxDetectorService {
  OrtSession? _session;
  OrtSession? _classifierSession;
  List<String> _labels = [];

  static const int inputSize = 640;
  static const int classifierInputSize = 224; // MobileNetV2 input size
  static const double confidenceThreshold = 0.5;
  static const double iouThreshold = 0.45;

  bool get isInitialized => _session != null;
  bool get hasLabels => _labels.isNotEmpty;

  /// Kh·ªüi t·∫°o ONNX model
  Future<void> initialize() async {
    try {
      OrtEnv.instance.init();

      final modelExists = await _checkAssetExists('assets/models/yolov8n.onnx');
      if (!modelExists) {
        throw Exception(
            'Kh√¥ng t√¨m th·∫•y file model trong assets/models/yolov8n.onnx');
      }

      final modelPath = await _copyAssetToFile('assets/models/yolov8n.onnx');

      final sessionOptions = OrtSessionOptions()
        ..setInterOpNumThreads(1)
        ..setIntraOpNumThreads(1)
        ..setSessionGraphOptimizationLevel(GraphOptimizationLevel.ortEnableAll);

      _session = OrtSession.fromFile(File(modelPath), sessionOptions);

      _labels = await _loadLabels();

      // Kh·ªüi t·∫°o MobileNetV2 classifier
      await _initializeClassifier();

      print('Model kh·ªüi t·∫°o th√†nh c√¥ng');
      print('S·ªë l∆∞·ª£ng nh√£n: ${_labels.length}');
    } catch (e) {
      print('L·ªói kh·ªüi t·∫°o model: $e');
      rethrow;
    }
  }

  /// Kh·ªüi t·∫°o MobileNetV2 classifier
  Future<void> _initializeClassifier() async {
    try {
      print('üîç ƒêang ki·ªÉm tra classifier model...');

      final classifierExists = await _checkAssetExists(
          'assets/models/mobilenetv2_speedlimit_v9.onnx');
      if (!classifierExists) {
        print('‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y classifier model');
        return;
      }

      print('‚úÖ T√¨m th·∫•y classifier model, ƒëang load...');

      final classifierPath = await _copyAssetToFile(
          'assets/models/mobilenetv2_speedlimit_v9.onnx');
      print('üìÅ Classifier path: $classifierPath');

      final sessionOptions = OrtSessionOptions()
        ..setInterOpNumThreads(1)
        ..setIntraOpNumThreads(1)
        ..setSessionGraphOptimizationLevel(GraphOptimizationLevel.ortEnableAll);

      _classifierSession =
          OrtSession.fromFile(File(classifierPath), sessionOptions);

      print('‚úÖ MobileNetV2 classifier kh·ªüi t·∫°o th√†nh c√¥ng');
      print(
          'üéØ Classifier session: ${_classifierSession != null ? "OK" : "NULL"}');
    } catch (e) {
      print('‚ùå L·ªói kh·ªüi t·∫°o classifier: $e');
      print('üìã Stack trace: ${StackTrace.current}');
    }
  }

  /// Ki·ªÉm tra asset c√≥ t·ªìn t·∫°i kh√¥ng
  Future<bool> _checkAssetExists(String assetPath) async {
    try {
      await rootBundle.load(assetPath);
      return true;
    } catch (e) {
      return false;
    }
  }

  /// Copy asset file to temporary directory
  Future<String> _copyAssetToFile(String assetPath) async {
    final byteData = await rootBundle.load(assetPath);
    final buffer = byteData.buffer;

    final tempDir = await getTemporaryDirectory();
    final fileName = path.basename(assetPath);
    final filePath = path.join(tempDir.path, fileName);

    final file = File(filePath);
    await file.writeAsBytes(
      buffer.asUint8List(byteData.offsetInBytes, byteData.lengthInBytes),
    );

    return filePath;
  }

  /// Load labels from assets
  Future<List<String>> _loadLabels() async {
    try {
      final labelsData =
          await rootBundle.loadString('assets/models/labels.txt');
      final labels = labelsData
          .split('\n')
          .map((e) => e.trim())
          .where((label) => label.isNotEmpty)
          .toList();

      if (labels.isEmpty) {
        print('C·∫£nh b√°o: File labels.txt tr·ªëng');
      }

      return labels;
    } catch (e) {
      print('C·∫£nh b√°o: Kh√¥ng th·ªÉ load file labels.txt - $e');
      return [];
    }
  }

  /// Ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng trong ·∫£nh
  Future<List<DetectionResult>> detectObjects(File imageFile) async {
    print('üîç B·∫Øt ƒë·∫ßu detect t·ª´ file: ${imageFile.path}');

    if (_session == null) {
      throw Exception('Model ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o');
    }

    if (!hasLabels) {
      throw Exception(
          'Ch∆∞a c√≥ file labels.txt. Vui l√≤ng th√™m file labels.txt v√†o assets/models/');
    }

    final imageBytes = await imageFile.readAsBytes();
    print('üì∏ ƒê·ªçc ƒë∆∞·ª£c ${imageBytes.length} bytes');

    final image = img.decodeImage(imageBytes);

    if (image == null) {
      throw Exception('Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh');
    }

    final originalWidth = image.width;
    final originalHeight = image.height;
    print('üñºÔ∏è K√≠ch th∆∞·ªõc ·∫£nh g·ªëc: ${originalWidth}x${originalHeight}');

    // Resize v·ªÅ model input size (640x640)
    final resizedImage = img.copyResize(
      image,
      width: inputSize,
      height: inputSize,
      interpolation: img.Interpolation.linear,
    );
    print(
        'üìè K√≠ch th∆∞·ªõc ·∫£nh sau resize: ${resizedImage.width}x${resizedImage.height}');
    print('‚öôÔ∏è Model input size: ${inputSize}x${inputSize}');

    // Preprocess resized image
    final inputData = _preprocessImage(resizedImage);

    // Create input tensor
    final inputOrt = OrtValueTensor.createTensorWithDataList(
      inputData,
      [1, 3, inputSize, inputSize],
    );

    // Run inference
    final inputs = {'images': inputOrt};
    final runOptions = OrtRunOptions();

    List<OrtValue?>? outputs;
    try {
      print('ü§ñ ƒêang ch·∫°y model inference...');
      outputs = _session!.run(runOptions, inputs);
      print('‚úÖ Model ch·∫°y xong');
    } catch (e) {
      print('‚ùå L·ªói khi ch·∫°y model: $e');
      inputOrt.release();
      runOptions.release();
      throw Exception('L·ªói khi ch·∫°y model: $e');
    }

    if (outputs.isEmpty) {
      inputOrt.release();
      runOptions.release();
      return [];
    }

    final outputTensor = outputs[0]?.value;

    // Release resources
    inputOrt.release();
    runOptions.release();
    outputs.forEach((element) => element?.release());

    if (outputTensor == null) {
      return [];
    }

    // Parse detections
    final detections = _parseYOLOv8Output(
      outputTensor,
      originalWidth,
      originalHeight,
    );

    print('üìä S·ªë detections tr∆∞·ªõc NMS: ${detections.length}');

    // Apply NMS
    final filteredDetections = _applyNMS(detections);

    print('‚ú® S·ªë detections sau NMS: ${filteredDetections.length}');

    // üéØ L·∫•y detection c√≥ confidence cao nh·∫•t ƒë·ªÉ ph√¢n lo·∫°i
    if (filteredDetections.isNotEmpty) {
      // T√¨m detection c√≥ conf cao nh·∫•t
      var bestDetection = filteredDetections[0];

      for (int i = 0; i < filteredDetections.length; i++) {
        if (filteredDetections[i].confidence > bestDetection.confidence) {
          bestDetection = filteredDetections[i];
        }
      }

      print('\nüéØ Detection t·ªët nh·∫•t ƒë∆∞·ª£c ch·ªçn:');
      print('   üìä Confidence: ${bestDetection.confidence}');
      print(
          '   üì¶ BBox: (${bestDetection.boundingBox.x.toInt()}, ${bestDetection.boundingBox.y.toInt()}, ${bestDetection.boundingBox.width.toInt()}, ${bestDetection.boundingBox.height.toInt()})');

      // Classify n·∫øu c√≥ classifier
      if (_classifierSession != null) {
        print('   ü§ñ B·∫Øt ƒë·∫ßu ph√¢n lo·∫°i t·ªëc ƒë·ªô...');

        final speedClass = await _classifySpeedSign(
          image,
          bestDetection.boundingBox,
        );

        if (speedClass != null) {
          print('   üö¶ K·∫øt qu·∫£ ph√¢n lo·∫°i: $speedClass km/h');

          // T·∫°o detection m·ªõi v·ªõi label ƒë√£ ph√¢n lo·∫°i
          bestDetection = DetectionResult(
            label: speedClass,
            confidence: bestDetection.confidence,
            boundingBox: bestDetection.boundingBox,
          );

          print('   ‚úÖ ƒê√£ c·∫≠p nh·∫≠t label th√†nh: $speedClass');
        } else {
          print(
              '   ‚ö†Ô∏è Ph√¢n lo·∫°i kh√¥ng th√†nh c√¥ng, gi·ªØ label m·∫∑c ƒë·ªãnh: ${bestDetection.label}');
        }
      } else {
        print('   ‚ö†Ô∏è Classifier ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o, b·ªè qua ph√¢n lo·∫°i');
      }

      // CH·ªà TR·∫¢ V·ªÄ 1 DETECTION T·ªëT NH·∫§T
      return [bestDetection];
    }

    return filteredDetections;
  }

  /// Preprocess image: normalize to [0, 1] and convert to CHW format
  Float32List _preprocessImage(img.Image image) {
    final imageData = Float32List(1 * 3 * inputSize * inputSize);

    int pixelIndex = 0;
    for (int c = 0; c < 3; c++) {
      for (int y = 0; y < inputSize; y++) {
        for (int x = 0; x < inputSize; x++) {
          final pixel = image.getPixel(x, y);
          double value;

          if (c == 0) {
            value = pixel.r / 255.0;
          } else if (c == 1) {
            value = pixel.g / 255.0;
          } else {
            value = pixel.b / 255.0;
          }

          imageData[pixelIndex++] = value;
        }
      }
    }

    return imageData;
  }

  /// Parse model outputs to detection results
  List<DetectionResult> _parseYOLOv8Output(
    dynamic outputTensor,
    int imageWidth,
    int imageHeight,
  ) {
    final detections = <DetectionResult>[];

    try {
      print('üîé Output type: ${outputTensor.runtimeType}');

      if (outputTensor is! List || outputTensor.isEmpty) {
        return [];
      }

      final batch = outputTensor[0];
      if (batch is! List || batch.isEmpty) {
        return [];
      }

      print('üì¶ Batch length: ${batch.length}');

      // Expecting format: [5, N] where N is number of detections
      // features[0] = x_center, features[1] = y_center,
      // features[2] = width, features[3] = height, features[4] = confidence

      if (batch.length < 5) {
        print('‚ùå Invalid output format - expected 5 rows');
        return [];
      }

      final xCenters = batch[0] as List;
      final yCenters = batch[1] as List;
      final widths = batch[2] as List;
      final heights = batch[3] as List;
      final confidences = batch[4] as List;

      final numBoxes = xCenters.length;
      print('üìä S·ªë boxes: $numBoxes');

      // ‚úÖ DEBUG: IN RA 10 GI√Å TR·ªä CONFIDENCE ƒê·∫¶U TI√äN
      print('üîç Sample confidences (first 10):');
      for (int i = 0; i < 10 && i < confidences.length; i++) {
        print('   conf[$i] = ${confidences[i]}');
      }

      // ‚úÖ DEBUG: T√åM MAX CONFIDENCE
      double maxConf = 0.0;
      int maxConfIndex = -1;
      for (int i = 0; i < confidences.length; i++) {
        final conf = (confidences[i] as num).toDouble();
        if (conf > maxConf) {
          maxConf = conf;
          maxConfIndex = i;
        }
      }
      print('üìä Max confidence found: $maxConf at index $maxConfIndex');
      print('üìä Confidence threshold: $confidenceThreshold');

      // üéØ Calculate scale factors (originalSize / modelInputSize)
      final scaleX = imageWidth / inputSize;
      final scaleY = imageHeight / inputSize;
      print('üìê Scale factors: scaleX=$scaleX, scaleY=$scaleY');
      print('üìê Model input size: ${inputSize}x${inputSize}');

      for (int i = 0; i < numBoxes; i++) {
        final xCenter = (xCenters[i] as num).toDouble();
        final yCenter = (yCenters[i] as num).toDouble();
        final width = (widths[i] as num).toDouble();
        final height = (heights[i] as num).toDouble();
        final conf = (confidences[i] as num).toDouble();

        // üîç B∆Ø·ªöC 1: LOG DEBUG CHO HIGH CONFIDENCE DETECTIONS
        if (conf > 0.5) {
          print('\nüéØ HIGH CONF DETECTION [$i]:');
          print('   üìä Confidence: $conf');
          print(
              '   üìê Model output coords: xC=$xCenter, yC=$yCenter, w=$width, h=$height');
          print('   üìè Original image size: ${imageWidth}x${imageHeight}');
          print('   ‚öôÔ∏è Model input was: ${inputSize}x${inputSize}');
          print('   üìä Scale factors: scaleX=$scaleX, scaleY=$scaleY');
        }

        // Filter by confidence threshold
        if (conf < confidenceThreshold) {
          if (conf > 0.5) {
            print('   ‚ùå SKIP: conf ($conf) < threshold ($confidenceThreshold)');
          }
          continue;
        }

        // üéØ AUTO-DETECT: Normalized [0,1] vs Pixel units
        final bool isNormalized =
            (xCenter <= 1.0 && yCenter <= 1.0 && width <= 1.0 && height <= 1.0);

        double x1, y1, x2, y2;

        if (isNormalized) {
          // Case 1: Normalized coordinates [0, 1]
          // Denormalize by multiplying with original image dimensions
          x1 = (xCenter - width / 2) * imageWidth;
          y1 = (yCenter - height / 2) * imageHeight;
          x2 = (xCenter + width / 2) * imageWidth;
          y2 = (yCenter + height / 2) * imageHeight;

          if (conf > 0.5) {
            print('   üîÑ Using NORMALIZED denormalization (coords in [0,1])');
            print('   üìç Denormalized bbox: x1=$x1, y1=$y1, x2=$x2, y2=$y2');
          }
        } else {
          // Case 2: Pixel units relative to model input size (640x640)
          // Scale from model space to original image space
          x1 = (xCenter - width / 2) * scaleX;
          y1 = (yCenter - height / 2) * scaleY;
          x2 = (xCenter + width / 2) * scaleX;
          y2 = (yCenter + height / 2) * scaleY;

          if (conf > 0.5) {
            print('   üîÑ Using SCALE denormalization (coords in pixel units)');
            print(
                '   üìç Before scale: x1=${xCenter - width / 2}, y1=${yCenter - height / 2}');
            print('   üìç After scale: x1=$x1, y1=$y1, x2=$x2, y2=$y2');
          }
        }

        // üîç B∆Ø·ªöC 2: KI·ªÇM TRA T·ª™NG ƒêI·ªÄU KI·ªÜN LO·∫†I B·ªé
        if (conf > 0.5) {
          if (x1 < 0) print('   ‚ö†Ô∏è WARNING: x1 ($x1) < 0');
          if (y1 < 0) print('   ‚ö†Ô∏è WARNING: y1 ($y1) < 0');
          if (x2 > imageWidth)
            print('   ‚ö†Ô∏è WARNING: x2 ($x2) > imageWidth ($imageWidth)');
          if (y2 > imageHeight)
            print('   ‚ö†Ô∏è WARNING: y2 ($y2) > imageHeight ($imageHeight)');
        }

        // Clamp to image bounds
        final clampedX1 = x1.clamp(0.0, imageWidth.toDouble());
        final clampedY1 = y1.clamp(0.0, imageHeight.toDouble());
        final clampedX2 = x2.clamp(0.0, imageWidth.toDouble());
        final clampedY2 = y2.clamp(0.0, imageHeight.toDouble());

        // Calculate final width and height
        final finalWidth = clampedX2 - clampedX1;
        final finalHeight = clampedY2 - clampedY1;

        // üîç B∆Ø·ªöC 2: KI·ªÇM TRA BBOX SIZE
        if (conf > 0.5) {
          print(
              '   üì¶ Final bbox after clamp: x=$clampedX1, y=$clampedY1, w=$finalWidth, h=$finalHeight');
          if (finalWidth < 10)
            print('   ‚ö†Ô∏è WARNING: finalWidth ($finalWidth) < 10');
          if (finalHeight < 10)
            print('   ‚ö†Ô∏è WARNING: finalHeight ($finalHeight) < 10');
        }

        if (finalWidth <= 0 || finalHeight <= 0) {
          if (conf > 0.5) {
            print('   ‚ùå SKIP: finalWidth or finalHeight <= 0');
          }
          continue;
        }

        // Use class ID 0 for now (all speed signs)
        final classId = 0;

        if (classId >= 0 && classId < _labels.length) {
          detections.add(
            DetectionResult(
              label: _labels[classId],
              confidence: conf,
              boundingBox: BoundingBox(
                x: clampedX1,
                y: clampedY1,
                width: finalWidth,
                height: finalHeight,
              ),
            ),
          );

          if (conf > 0.5) {
            print('   ‚úÖ ADDED to detections list!');
          }

          if (i < 5) {
            print(
                'üéØ Detection $i: conf=$conf, box=(${clampedX1.toInt()},${clampedY1.toInt()},${finalWidth.toInt()},${finalHeight.toInt()})');
          }
        } else {
          if (conf > 0.5) {
            print(
                '   ‚ùå SKIP: classId ($classId) out of range (labels: ${_labels.length})');
          }
        }
      }
    } catch (e) {
      print('‚ùå L·ªói parse output: $e');
    }

    return detections;
  }

  /// Apply Non-Maximum Suppression
  List<DetectionResult> _applyNMS(List<DetectionResult> detections) {
    if (detections.isEmpty) return [];

    // Sort descending by confidence (best first)
    detections.sort((a, b) => b.confidence.compareTo(a.confidence));

    final selected = <DetectionResult>[];
    final remaining = List<DetectionResult>.from(detections);

    while (remaining.isNotEmpty) {
      // Pick the best (highest confidence)
      final best = remaining.removeAt(0);
      selected.add(best);

      // Remove all boxes with high IoU with the best box
      remaining.removeWhere((box) {
        final iou = best.boundingBox.iou(box.boundingBox);
        return iou > iouThreshold;
      });
    }

    return selected;
  }

  /// Ph√¢n lo·∫°i t·ªëc ƒë·ªô t·ª´ v√πng detection
  Future<String?> _classifySpeedSign(
    img.Image originalImage,
    BoundingBox bbox,
  ) async {
    if (_classifierSession == null) {
      print('‚ö†Ô∏è Classifier ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o');
      return null;
    }

    try {
      // 1. Crop v√πng detection
      final x = bbox.x.toInt().clamp(0, originalImage.width - 1);
      final y = bbox.y.toInt().clamp(0, originalImage.height - 1);
      final w = bbox.width.toInt().clamp(1, originalImage.width - x);
      final h = bbox.height.toInt().clamp(1, originalImage.height - y);

      final croppedImage = img.copyCrop(
        originalImage,
        x: x,
        y: y,
        width: w,
        height: h,
      );

      print('   ‚úÇÔ∏è Cropped: ${croppedImage.width}x${croppedImage.height}');

      // 2. Resize v·ªÅ 224x224 (MobileNetV2 input)
      final resizedImage = img.copyResize(
        croppedImage,
        width: classifierInputSize,
        height: classifierInputSize,
        interpolation: img.Interpolation.linear,
      );

      // 3. Convert sang grayscale
      final grayscaleImage = img.grayscale(resizedImage);

      print(
          '   üé® Converted to grayscale: ${grayscaleImage.width}x${grayscaleImage.height}');

      // 4. Preprocess cho MobileNetV2 (Grayscale duplicate th√†nh 3 channels)
      final inputData = _preprocessClassifierImage(grayscaleImage);

      // 5. Create input tensor [1, 3, 224, 224] (3 channels v·ªõi gi√° tr·ªã gi·ªëng nhau)
      final inputOrt = OrtValueTensor.createTensorWithDataList(
        inputData,
        [1, 3, classifierInputSize, classifierInputSize],
      );

      // 6. Run inference
      final inputs = {'input': inputOrt};
      final runOptions = OrtRunOptions();

      print('   ü§ñ ƒêang ch·∫°y MobileNetV2...');
      final outputs = _classifierSession!.run(runOptions, inputs);

      // Cleanup
      inputOrt.release();
      runOptions.release();

      if (outputs.isEmpty) {
        outputs.forEach((element) => element?.release());
        return null;
      }

      final outputTensor = outputs[0]?.value;
      outputs.forEach((element) => element?.release());

      if (outputTensor == null) return null;

      // 7. Parse output
      final speedClass = _parseClassifierOutput(outputTensor);

      return speedClass;
    } catch (e) {
      print('‚ùå L·ªói classification: $e');
      return null;
    }
  }

  /// Preprocess image cho classifier (Grayscale duplicate to 3 channels, CHW, normalize)
  Float32List _preprocessClassifierImage(img.Image image) {
    final imageData =
        Float32List(1 * 3 * classifierInputSize * classifierInputSize);

    int pixelIndex = 0;
    // Grayscale duplicate th√†nh 3 channels (R=G=B) ƒë·ªÉ model nh·∫≠n 3 channels
    for (int c = 0; c < 3; c++) {
      for (int y = 0; y < classifierInputSize; y++) {
        for (int x = 0; x < classifierInputSize; x++) {
          final pixel = image.getPixel(x, y);
          // Grayscale: r == g == b, duplicate c√πng gi√° tr·ªã cho c·∫£ 3 channels
          final value = pixel.r / 255.0;
          imageData[pixelIndex++] = value;
        }
      }
    }

    return imageData;
  }

  /// Parse classifier output ƒë·ªÉ l·∫•y speed class
  String? _parseClassifierOutput(dynamic outputTensor) {
    try {
      // Output shape: [1, 11] - 11 classes
      // ['100', '110', '120', '20', '30', '40', '50', '60', '70', '80', '90']

      List<double> scores;

      if (outputTensor is List) {
        if (outputTensor.isEmpty) return null;

        // Flatten if nested
        dynamic flatList = outputTensor;
        while (flatList is List && flatList.isNotEmpty && flatList[0] is List) {
          flatList = flatList[0];
        }

        scores = (flatList as List).map((e) => (e as num).toDouble()).toList();
      } else {
        return null;
      }

      if (scores.isEmpty || scores.length != _labels.length) {
        print(
            '‚ùå Invalid output: ${scores.length} scores, expected ${_labels.length}');
        return null;
      }

      // T√¨m class c√≥ score cao nh·∫•t
      double maxScore = scores[0];
      int maxIndex = 0;

      for (int i = 1; i < scores.length; i++) {
        if (scores[i] > maxScore) {
          maxScore = scores[i];
          maxIndex = i;
        }
      }

      print('   üìä Classification scores: ${scores.take(5).toList()}...');
      print('   üèÜ Best class: ${_labels[maxIndex]} (score: $maxScore)');

      return _labels[maxIndex];
    } catch (e) {
      print('‚ùå L·ªói parse classifier output: $e');
      return null;
    }
  }

  /// Dispose resources
  void dispose() {
    _session?.release();
    _classifierSession?.release();
    _session = null;
    _classifierSession = null;
    OrtEnv.instance.release();
  }
}
